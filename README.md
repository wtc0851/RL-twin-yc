# 双场桥调度强化学习环境

## 1. 项目概述

本项目为模拟集装箱码头中的双轨道式堆场起重机（场桥）调度优化问题，提供了一个强化学习（RL）环境。其主要目标是训练一个强化学习代理，使其能够做出智能的调度决策，将集装箱处理任务分配给两台起重机，以在确保无碰撞操作的同时，最大限度地提升作业效率。

## 2. 仿真环境

该仿真环境模拟了集装箱堆场的关键操作方面。

### 2.1. 堆场布局与起重机
- **堆场区：** 一个由50个标准集装箱贝位组成的线性区域。
- **场桥（ARMG）：** 两台起重机（ARMG1, ARMG2）在此区域内的同一组轨道上运行。
- **核心约束：** 两台起重机共享一条轨道，**不能相互越过**。ARMG1 必须始终在 ARMG2 的左侧（或同一位置）。任何会导致它们的路径交叉或位置重叠的操作计划都将被视为碰撞。

### 2.2. 任务处理
- **任务生成：** 动态生成集装箱处理任务，模拟外部车辆的到达。
- **任务属性：**
    - **位置（贝位）：** 必须执行集装箱处理操作的特定贝位。
    - **可用时间：** 任务可以开始的最早时间。
    - **执行时间：** 起重机在现场完成处理操作所需的时间。

## 3. 强化学习实现

该问题被建模为一个马尔可夫决策过程（MDP），并使用 `sb3-contrib` 库中的 `MaskablePPO` 算法进行训练。

### 3.1. 状态空间 (Observation Space)

在每个决策点，代理观察一个包含四部分信息的字典 (`gym.spaces.Dict`)：

1.  **`crane_status` (`Box(2, 2)`)**: 一个 `2x2` 的矩阵，表示两台场桥的状态。
    - `[i, 0]`: 第 `i` 台场桥的当前贝位。
    - `[i, 1]`: 第 `i` 台场桥的当前状态 (`IDLE`, `MOVING`, `BUSY`) 对应的枚举值。

2.  **`task_list` (`Box(10, 3)`)**: 一个 `10x3` 的矩阵，表示按创建时间排序的、最早的10个待处理任务。
    - `[j, 0]`: 第 `j` 个任务的目标贝位。
    - `[j, 1]`: 第 `j` 个任务的可用时间。
    - `[j, 2]`: 第 `j` 个任务的预计执行时间。

3.  **`action_mask` (`Box(11,)`)**: 一个长度为11的二进制向量，由环境提供，用于屏蔽无效动作。值为1表示有效，0表示无效。

4.  **`crane_to_command` (`Box(2,)`)**: 一个 one-hot 向量，指示当前需要哪一台场桥做出决策。例如 `[1, 0]` 表示 ARMG1 需要决策。

### 3.2. 动作空间 (Action Space)

代理的动作为 `gym.spaces.Discrete(11)`，含义如下：
- **动作 `0` 到 `9`**: 对应于将 `task_list` 中相应索引的任务分配给正在决策的场桥。
- **动作 `10`**: 让场桥执行“等待”动作，不分配任何任务。

### 3.3. 核心机制：预判式动作掩码 (Predictive Action Masking)

为了从根本上杜绝碰撞，我们设计了一种严格的**预判式动作掩码**机制。其核心思想是：**在决策时，就屏蔽掉所有未来可能导致碰撞的动作。**

该机制遵循以下逻辑：

1.  **动态障碍物**：在为一台场桥（`commanding_crane`）决策时，另一台场桥（`other_crane`）被视为一个动态障碍物。
    - 如果 `other_crane` **静止**，它的当前位置就是障碍点。
    - 如果 `other_crane` **正在移动**，它从当前位置到其任务目的地的**整个路径**都被视为一个动态的“禁区”。

2.  **路径预判与屏蔽**：对于每一个待考虑的候选任务：
    - 计算 `commanding_crane` 执行该任务所需要行走的路径。
    - **检查该路径是否会与 `other_crane` 的“禁区”发生重叠或穿越。**
    - 任何会导致路径交叉的动作都会在动作掩码中被置为 `0`，从而被禁止选择。

这种“事前防范”的机制，通过规则保证了智能体无法做出危险决策，是整个系统安全运行的基石。

### 3.4. 奖励机制 (Reward Mechanism)

我们设计了一套分阶段的奖励函数来引导智能体学习高效且安全的策略：

- **`+50` (任务接受)**: 当一个有效的任务被成功分配时。
- **`+200` (到达现场)**: 当场桥移动到任务指定的贝位时。
- **`+750 - (等待时间)` (完成任务)**: 当任务被最终完成时，会获得一个基础奖励，并减去从任务生成到任务完成的总时间。这激励智能体最小化任务的等待时间。
- **`-10` (等待惩罚)**: 当智能体选择“等待”动作时，施加一个小的负惩罚，鼓励其在有安全选项时积极接受任务。
- **`-500` (碰撞/无效动作惩罚)**: 当智能体尝试一个被掩码屏蔽的无效动作时，给予巨大的负惩罚。

## 4. 如何运行

1.  **安装依赖**:
    ```bash
    pip install -r requirements.txt
    ```

2.  **训练模型**:
    ```bash
    python src/agents/train_ppo.py
    ```
    训练好的模型将保存在 `models/ppo_yard_model.zip`。

3.  **评估模型**:
    ```bash
    python tests/test_agent.py
    ```
    评估脚本会加载训练好的模型，在环境中运行一个完整的仿真回合，并生成一张名为 `trained_agent_trajectory.png` 的轨迹图，用于可视化智能体的行为。