# 双场桥调度强化学习环境

## 1. 项目概述

本项目为模拟集装箱码头中的双轨道式堆场起重机（场桥）调度优化问题，提供了一个强化学习（RL）环境。其主要目标是训练一个强化学习代理，使其能够做出智能的调度决策，将集装箱处理任务分配给两台起重机，以在确保无碰撞操作的同时，最大限度地减少任务的总等待时间。

## 2. 仿真环境

该仿真环境模拟了集装箱堆场的关键操作方面。

### 2.1. 堆场布局与起重机
- **堆场区：** 一个由50个标准集装箱贝位组成的线性区域。每个贝位可堆放4层高、6排深的集装箱。
- **堆场起重机（YCs）：** 两台起重机在此区域内的同一组轨道上运行。它们负责抓取、运输和放置集装箱。
- **核心约束：** 两台起重机共享一条轨道，**不能相互越过**。任何会导致它们的路径交叉或位置重叠的操作计划都将被视为碰撞，并导致操作立即失败。

### 2.2. 任务处理
- **任务生成：** 动态生成集装箱处理任务，模拟外部卡车或内部转运车辆的到达。连续任务到达之间的时间间隔服从均值约为5分钟的正态分布。所有新生成的任务都被放入一个中央任务池中。
- **任务属性：**
    - **位置（贝位）：** 必须执行集装箱处理操作的特定贝位。
    - **可用时间：** 任务可以开始的最早时间。这模拟了卡车从码头门口行驶到特定贝位所需的时间。
    - **执行时间：** 起重机在现场完成处理操作所需的时间。这是一个2到4分钟之间的随机整数。

### 2.3. 起重机操作
- **移动：** 起重机以每分钟10个贝位的恒定速度沿贝位移动。
- **执行：** 到达目标贝位后，起重机执行任务，耗时即为定义的“执行时间”。

## 3. 调度问题

### 3.1. 目标
代理的目标是在1小时的仿真周期内，**最大限度地减少**所有已完成任务的**总等待时间**。等待时间定义为：`(任务完成时间) - (任务可用时间)`。

### 3.2. 决策触发
仅当一台起重机完成当前任务并变为空闲状态时，才需要进行决策。

### 3.3. 动作
代理的动作是从中央任务池中选择一个任务，并将其分配给新近空闲的起重机。

### 3.4. 终止条件
在以下任一情况下，一个回合（episode）将终止：
- **时间限制：** 仿真时钟达到1小时。
- **碰撞：** 两台起重机相撞（即它们的位置重叠或交叉）。

## 4. 强化学习建模

该问题被建模为一个马尔可夫决策过程（MDP），包含以下组成部分：

### 4.1. 状态空间
在每个决策点，代理观察以下状态信息：
1.  **起重机状态：**
    - 每台起重机当前的贝位（1-50）。
    - 每台起重机的操作状态（`空闲`、`移动中`、`繁忙`）。
    - 每台非空闲起重机预计将空闲的时间。
2.  **任务池状态：**
    - 一个表示待处理任务的固定长度向量。每个条目包括任务的**位置**、**可用时间**和**执行时间**。空槽用零填充。
3.  **全局状态：**
    - 当前的仿真时间。

### 4.2. 动作空间
为了处理可变数量的任务，采用了**固定大小的动作空间与动作掩码**相结合的方法。
- **动作空间：** 定义了一个离散的动作空间 `A = {0, 1, ..., N-1}`，其中 `N` 是任务池的最大容量。动作 `i` 对应于将池中的第 `i` 个任务分配给空闲的起重机。可以包含一个额外的动作来表示“等待”（即不分配任何任务）。
- **动作掩码：** 在每个决策步骤，环境提供一个与动作空间相对应的二进制掩码。该掩码指示哪些动作当前是有效的（即哪些任务在池中可用）。代理使用此掩码来确保它只从有效动作中进行选择。

### 4.3. 奖励函数
奖励函数引导代理的学习过程：
- **主要奖励：** 完成一个任务后，代理会收到一个等于该任务等待时间（`完成时间 - 可用时间`）的负奖励。最大化累积奖励等同于最小化总等待时间。
- **惩罚：**
    - **碰撞惩罚：** 如果发生碰撞，将给予一个巨大的负奖励（例如-1000），并立即终止该回合。
    - **无效动作惩罚：** 如果代理尝试无效的动作，可以施加一个小的负惩罚（尽管这通常通过动作掩码来防止）。